# utcp-mcp-gateway

<p align="center">
  <strong>ğŸš€ Save 90%+ Token Cost with Smart MCP Proxy</strong>
</p>

<p align="center">
  <a href="#english">English</a> | <a href="#ä¸­æ–‡">ä¸­æ–‡</a>
</p>

---

<a name="english"></a>

## What is this?

**The Problem:** MCP tools often return huge responses (10,000+ chars), wasting your LLM tokens.

**The Solution:** `utcp-mcp-gateway` acts as a smart proxy that:
1. Connects to ANY MCP server (HTTP or stdio)
2. Filters responses with LLM summarization
3. Returns only what matters (saving 90%+ tokens!)

```
Your AI  â†’  utcp-mcp-gateway  â†’  Any MCP Server
              â†“
         LLM Filter (97% smaller responses!)
```

## Quick Start

**Zero config files!** Just add to Claude Desktop:

```json
{
  "mcpServers": {
    "gateway": {
      "command": "npx",
      "args": ["-y", "utcp-mcp-gateway"],
      "env": {
        "MCP_URL": "https://mcp.context7.com/mcp",
        "MCP_NAME": "context7",
        "LLM_API_KEY": "sk-xxx"
      }
    }
  }
}
```

That's it! Your AI now has access to Context7 with smart filtering.

## Why Code Mode?

> *"LLMs excel at writing code but struggle with tool calls."*  
> â€” Apple, Cloudflare, Anthropic

**Traditional Tool Calling:**
```
User â†’ LLM â†’ Tool 1 â†’ LLM â†’ Tool 2 â†’ LLM â†’ Tool 3 â†’ Result
       (5 round trips, massive token waste)
```

**Code Mode:**
```
User â†’ LLM writes code â†’ Execute all tools at once â†’ Result
       (1 round trip, 60%+ token savings)
```

### Benchmark Results

| Metric | Traditional | Code Mode | Savings |
|--------|-------------|-----------|---------|
| API Calls | 15+ calls | 1 call | **93%** |
| Token Cost | $26/day | $0.87/day | **$9,536/year** |
| Latency | 5+ round trips | 1 round trip | **80%** |

*Source: [Independent Python Benchmark](https://github.com/imran31415/codemode_python_benchmark)*

## Features

| Feature | Description |
|---------|-------------|
| ğŸ”Œ **Universal MCP** | Connect any HTTP or stdio MCP server |
| ğŸ§  **LLM Filtering** | Intelligent summarization (97% response reduction!) |
| ğŸ” **Progressive Discovery** | `search_tools` - find tools without loading all 500 definitions |
| âš¡ **Code Mode** | Execute TypeScript tool chains in one call |
| ğŸ”’ **Secure Sandbox** | Code runs in isolated environment |
| ğŸ“¦ **Zero Config** | Environment variables only, no config files |

## Token Savings Benchmarks

| MCP Service | Original | Filtered | Savings |
|-------------|----------|----------|---------|
| Context7 (docs) | 10,625 chars | 326 chars | **97%** |
| DeepWiki (wiki) | 3,318 chars | 400 chars | **88%** |

## Configuration

### Single MCP

```bash
MCP_URL=https://mcp.context7.com/mcp
MCP_NAME=context7
```

### Multiple MCPs

```bash
MCP_URLS=https://mcp.context7.com/mcp,https://mcp.deepwiki.com/mcp
MCP_NAMES=context7,deepwiki
```

### LLM Settings

| Variable | Default | Description |
|----------|---------|-------------|
| `LLM_API_KEY` | - | OpenAI/OpenRouter API key |
| `LLM_BASE_URL` | OpenAI | Custom endpoint (OpenAI-compatible) |
| `LLM_MODEL` | gpt-4o-mini | Model for summarization |
| `ENABLE_LLM_FILTER` | true | Enable/disable filtering |
| `MAX_RESPONSE_CHARS` | 2000 | Max response length |

## Exposed Tools

| Tool | Description |
|------|-------------|
| `search_tools` | Find tools by keyword (progressive discovery) |
| `list_tools` | List all available tools |
| `call_tool` | Call any tool with smart filtering |
| `call_tool_chain` | Execute TypeScript code chains |

---

<a name="ä¸­æ–‡"></a>

## è¿™æ˜¯ä»€ä¹ˆï¼Ÿ

**é—®é¢˜ï¼š** MCP å·¥å…·ç»å¸¸è¿”å›å·¨å¤§çš„å“åº”ï¼ˆ10,000+ å­—ç¬¦ï¼‰ï¼Œæµªè´¹ä½ çš„ LLM Tokenã€‚

**è§£å†³æ–¹æ¡ˆï¼š** `utcp-mcp-gateway` ä½œä¸ºæ™ºèƒ½ä»£ç†ï¼š
1. è¿æ¥ä»»æ„ MCP æœåŠ¡å™¨ï¼ˆHTTP æˆ– stdioï¼‰
2. ç”¨ LLM æ™ºèƒ½è¿‡æ»¤å“åº”
3. åªè¿”å›é‡è¦ä¿¡æ¯ï¼ˆèŠ‚çœ 90%+ Tokenï¼ï¼‰

```
ä½ çš„ AI  â†’  utcp-mcp-gateway  â†’  ä»»æ„ MCP æœåŠ¡
                â†“
           LLM è¿‡æ»¤ï¼ˆå“åº”ç¼©å° 97%ï¼ï¼‰
```

## å¿«é€Ÿå¼€å§‹

**é›¶é…ç½®æ–‡ä»¶ï¼** ç›´æ¥æ·»åŠ åˆ° Claude Desktopï¼š

```json
{
  "mcpServers": {
    "gateway": {
      "command": "npx",
      "args": ["-y", "utcp-mcp-gateway"],
      "env": {
        "MCP_URL": "https://mcp.context7.com/mcp",
        "MCP_NAME": "context7",
        "LLM_API_KEY": "sk-xxx"
      }
    }
  }
}
```

å°±è¿™æ ·ï¼ä½ çš„ AI ç°åœ¨å¯ä»¥ä½¿ç”¨å¸¦æ™ºèƒ½è¿‡æ»¤çš„ Context7 äº†ã€‚

## ä¸ºä»€ä¹ˆç”¨ Code Modeï¼Ÿ

> *"LLM æ“…é•¿å†™ä»£ç ï¼Œä½†ä¸æ“…é•¿è°ƒç”¨å·¥å…·ã€‚"*  
> â€” Apple, Cloudflare, Anthropic

**ä¼ ç»Ÿå·¥å…·è°ƒç”¨ï¼š**
```
ç”¨æˆ· â†’ LLM â†’ å·¥å…·1 â†’ LLM â†’ å·¥å…·2 â†’ LLM â†’ å·¥å…·3 â†’ ç»“æœ
       (5 æ¬¡å¾€è¿”ï¼Œå¤§é‡ Token æµªè´¹)
```

**Code Modeï¼š**
```
ç”¨æˆ· â†’ LLM å†™ä»£ç  â†’ ä¸€æ¬¡æ‰§è¡Œæ‰€æœ‰å·¥å…· â†’ ç»“æœ
       (1 æ¬¡å¾€è¿”ï¼ŒèŠ‚çœ 60%+ Token)
```

### æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | ä¼ ç»Ÿæ–¹å¼ | Code Mode | èŠ‚çœ |
|------|----------|-----------|------|
| API è°ƒç”¨æ¬¡æ•° | 15+ æ¬¡ | 1 æ¬¡ | **93%** |
| Token æˆæœ¬ | $26/å¤© | $0.87/å¤© | **$9,536/å¹´** |
| å»¶è¿Ÿ | 5+ æ¬¡å¾€è¿” | 1 æ¬¡å¾€è¿” | **80%** |

*æ•°æ®æ¥æº: [ç‹¬ç«‹ Python åŸºå‡†æµ‹è¯•](https://github.com/imran31415/codemode_python_benchmark)*

## æ ¸å¿ƒåŠŸèƒ½

| åŠŸèƒ½ | è¯´æ˜ |
|------|------|
| ğŸ”Œ **é€šç”¨ MCP** | è¿æ¥ä»»æ„ HTTP æˆ– stdio MCP |
| ğŸ§  **LLM è¿‡æ»¤** | æ™ºèƒ½æ‘˜è¦ï¼ˆå“åº”ç¼©å° 97%ï¼ï¼‰|
| ğŸ” **æ¸è¿›å¼å‘ç°** | `search_tools` - æŒ‰éœ€æœç´¢ï¼Œæ— éœ€åŠ è½½å…¨éƒ¨ 500 ä¸ªå·¥å…· |
| âš¡ **Code Mode** | ä¸€æ¬¡è°ƒç”¨æ‰§è¡Œ TypeScript ä»£ç é“¾ |
| ğŸ”’ **å®‰å…¨æ²™ç®±** | ä»£ç åœ¨éš”ç¦»ç¯å¢ƒè¿è¡Œ |
| ğŸ“¦ **é›¶é…ç½®** | åªéœ€ç¯å¢ƒå˜é‡ï¼Œæ— éœ€é…ç½®æ–‡ä»¶ |

## Token èŠ‚çœå®æµ‹

| MCP æœåŠ¡ | åŸå§‹å“åº” | è¿‡æ»¤å | èŠ‚çœ |
|----------|----------|--------|------|
| Context7 | 10,625 å­—ç¬¦ | 326 å­—ç¬¦ | **97%** |
| DeepWiki | 3,318 å­—ç¬¦ | 400 å­—ç¬¦ | **88%** |

## é…ç½®è¯´æ˜

### å•ä¸ª MCP

```bash
MCP_URL=https://mcp.context7.com/mcp
MCP_NAME=context7
```

### å¤šä¸ª MCP

```bash
MCP_URLS=https://mcp.context7.com/mcp,https://mcp.deepwiki.com/mcp
MCP_NAMES=context7,deepwiki
```

### LLM é…ç½®

| å˜é‡ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `LLM_API_KEY` | - | OpenAI/OpenRouter API å¯†é’¥ |
| `LLM_BASE_URL` | OpenAI | è‡ªå®šä¹‰ç«¯ç‚¹ï¼ˆå…¼å®¹ OpenAI æ ¼å¼ï¼‰|
| `LLM_MODEL` | gpt-4o-mini | æ‘˜è¦ç”¨çš„æ¨¡å‹ |
| `ENABLE_LLM_FILTER` | true | å¼€å¯/å…³é—­è¿‡æ»¤ |
| `MAX_RESPONSE_CHARS` | 2000 | æœ€å¤§å“åº”é•¿åº¦ |

---

## Credits / è‡´è°¢

This project is built on top of amazing open-source work:

- **[UTCP (Universal Tool Calling Protocol)](https://github.com/anthropics/utcp)** - The protocol that makes this possible
- **[@utcp/code-mode](https://www.npmjs.com/package/@utcp/code-mode)** - Code execution capabilities
- **[@utcp/sdk](https://www.npmjs.com/package/@utcp/sdk)** - UTCP SDK
- **[@modelcontextprotocol/sdk](https://www.npmjs.com/package/@modelcontextprotocol/sdk)** - MCP SDK by Anthropic

## License

MIT Â© 2025 reinn
